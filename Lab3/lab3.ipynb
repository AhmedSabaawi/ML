{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_pickles_with_class(path, class_name):\n",
    "    \"\"\"\n",
    "    Combines acc and gyro pickle files for a given class into a single DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - path (str): Directory path containing the pickle files.\n",
    "    - class_name (str): The class name to filter files (e.g., 'jump').\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Combined DataFrame with acc, gyro data and class column.\n",
    "    \"\"\"\n",
    "    # Initialize lists to hold acc and gyro files\n",
    "    acc_files = []\n",
    "    gyro_files = []\n",
    "\n",
    "    # List all files in the directory\n",
    "    for file in os.listdir(path):\n",
    "        if file.startswith(class_name) and file.endswith(\"_acc.pkl\"):\n",
    "            acc_files.append(file)\n",
    "        elif file.startswith(class_name) and file.endswith(\"_gyro.pkl\"):\n",
    "            gyro_files.append(file)\n",
    "\n",
    "    # Create a list to hold the combined data for all the files\n",
    "    combined_data_list = []\n",
    "\n",
    "    # Rename columns appropriately for acc and gyro DataFrames\n",
    "    for i in range(len(acc_files)):\n",
    "        acc_df = pd.read_pickle(path + acc_files[i])\n",
    "        gyro_df = pd.read_pickle(path + gyro_files[i])\n",
    "\n",
    "        # Drop the first column (assumed to be an index or unneeded)\n",
    "\n",
    "        # if num of columns is 4, then drop the first column\n",
    "        if len(acc_df.columns) == 4:\n",
    "            acc_df.drop(acc_df.columns[0], axis=1, inplace=True)\n",
    "            gyro_df.drop(gyro_df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "        # Rename columns\n",
    "        acc_df.columns = [\"ax\", \"ay\", \"az\"]\n",
    "        gyro_df.columns = [\"gx\", \"gy\", \"gz\"]\n",
    "\n",
    "        # Combine acc and gyro DataFrames horizontally\n",
    "        combined_df = pd.concat([acc_df, gyro_df], axis=1)\n",
    "\n",
    "        # Add class column\n",
    "        combined_df[\"class\"] = class_name\n",
    "\n",
    "        # Append the combined DataFrame to the list\n",
    "        combined_data_list.append(combined_df)\n",
    "\n",
    "    # Concatenate all DataFrames vertically to get the final combined DataFrame\n",
    "    final_df = pd.concat(combined_data_list, ignore_index=True)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "# test the function\n",
    "# combine_pickles_with_class(\"clean_data/train/\", \"jump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train, valid, test\n",
    "data_type = \"train\"\n",
    "\n",
    "# # print the description of the data\n",
    "jump_data_cleaned = combine_pickles_with_class(f\"clean_data/{data_type}/\", \"jump\")\n",
    "walk_data_cleaned = combine_pickles_with_class(f\"clean_data/{data_type}/\", \"walk\")\n",
    "spain_data_cleaned = combine_pickles_with_class(f\"clean_data/{data_type}/\", \"spain\")\n",
    "run_data_cleaned = combine_pickles_with_class(f\"clean_data/{data_type}/\", \"run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle NaN and infinite values\n",
    "def clean_data(X_train, X_valid, X_test):\n",
    "    # Replace NaN and infinite values with mean values\n",
    "    X_train.fillna(X_train.mean(), inplace=True)\n",
    "    X_valid.fillna(X_valid.mean(), inplace=True)\n",
    "    X_test.fillna(X_test.mean(), inplace=True)\n",
    "    return X_train, X_valid, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_train = pd.read_pickle(\"clean_data/train/all_data_train.pkl\")\n",
    "data_valid = pd.read_pickle(\"clean_data/valid/all_data_valid.pkl\")\n",
    "data_test = pd.read_pickle(\"clean_data/test/all_data_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (y)\n",
    "X_train = data_train.drop(\"class\", axis=1)\n",
    "X_valid = data_valid.drop(\"class\", axis=1)\n",
    "X_test = data_test.drop(\"class\", axis=1)\n",
    "\n",
    "y_train = data_train[\"class\"]\n",
    "y_valid = data_valid[\"class\"]\n",
    "y_test = data_test[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target variable\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_y_train = encoder.transform(y_train)\n",
    "encoded_y_valid = encoder.transform(y_valid)\n",
    "encoded_y_test = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data (replace NaNs and infinities)\n",
    "X_train, X_valid, X_test = clean_data(X_train, X_valid, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaNs after scaling\n",
    "print(\"After cleaning:\")\n",
    "print(f\"X_train NaNs: {np.isnan(X_train).sum()}\")\n",
    "print(f\"X_valid NaNs: {np.isnan(X_valid).sum()}\")\n",
    "print(f\"X_test NaNs: {np.isnan(X_test).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the first model (10 neurons per layer)\n",
    "def create_model_10():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=6, activation=\"relu\"))\n",
    "    model.add(Dense(10, activation=\"relu\"))\n",
    "    model.add(Dense(4, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the second model (50 neurons per layer)\n",
    "def create_model_50():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=6, activation=\"relu\"))\n",
    "    model.add(Dense(50, activation=\"relu\"))\n",
    "\n",
    "    model.add(Dense(4, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with 10 neurons per layer\n",
    "model_10 = create_model_10()\n",
    "history_10 = model_10.fit(\n",
    "    X_train,\n",
    "    encoded_y_train,\n",
    "    epochs=100,\n",
    "    batch_size=10,\n",
    "    validation_data=(X_valid, encoded_y_valid),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with 50 neurons per layer\n",
    "model_50 = create_model_50()\n",
    "history_50 = model_50.fit(\n",
    "    X_train,\n",
    "    encoded_y_train,\n",
    "    epochs=100,\n",
    "    batch_size=10,\n",
    "    validation_data=(X_valid, encoded_y_valid),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both models on the test set\n",
    "print(\"\\nEvaluating Model with 10 Neurons:\")\n",
    "test_loss_10, test_accuracy_10 = model_10.evaluate(X_test, encoded_y_test)\n",
    "print(f\"Model 10 Neurons - Test Accuracy: {test_accuracy_10 * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nEvaluating Model with 50 Neurons:\")\n",
    "test_loss_50, test_accuracy_50 = model_50.evaluate(X_test, encoded_y_test)\n",
    "print(f\"Model 50 Neurons - Test Accuracy: {test_accuracy_50 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy and loss for both models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, model_name):\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "    plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "    plt.title(f\"{model_name} - Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    plt.title(f\"{model_name} - Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history for both models\n",
    "plot_history(history_10, \"Model with 10 Neurons\")\n",
    "plot_history(history_50, \"Model with 50 Neurons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both models on the test set\n",
    "print(\"\\nEvaluating Model with 10 Neurons:\")\n",
    "test_loss_10, test_accuracy_10 = model_10.evaluate(X_test, encoded_y_test)\n",
    "print(f\"Model 10 Neurons - Test Accuracy: {test_accuracy_10 * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nEvaluating Model with 50 Neurons:\")\n",
    "test_loss_50, test_accuracy_50 = model_50.evaluate(X_test, encoded_y_test)\n",
    "print(f\"Model 50 Neurons - Test Accuracy: {test_accuracy_50 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy and loss for both models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_history(history, model_name):\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "    plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "    plt.title(f\"{model_name} - Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    plt.title(f\"{model_name} - Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot training history for both models\n",
    "plot_history(history_10, \"Model with 10 Neurons\")\n",
    "plot_history(history_50, \"Model with 50 Neurons\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
