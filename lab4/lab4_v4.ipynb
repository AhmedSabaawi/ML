{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, Dropout\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "\n",
    "# Function to clean NaN and infinite values\n",
    "def clean_data(X_train, X_valid, X_test):\n",
    "    X_train.fillna(X_train.mean(), inplace=True)\n",
    "    X_valid.fillna(X_valid.mean(), inplace=True)\n",
    "    X_test.fillna(X_test.mean(), inplace=True)\n",
    "    return X_train, X_valid, X_test\n",
    "\n",
    "\n",
    "# Load data\n",
    "data_train = pd.read_pickle(\"clean_data/train/all_data_train.pkl\")\n",
    "data_valid = pd.read_pickle(\"clean_data/valid/all_data_valid.pkl\")\n",
    "data_test = pd.read_pickle(\"clean_data/test/all_data_test.pkl\")\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X_train = data_train.drop(\"class\", axis=1)\n",
    "X_valid = data_valid.drop(\"class\", axis=1)\n",
    "X_test = data_test.drop(\"class\", axis=1)\n",
    "\n",
    "y_train = data_train[\"class\"]\n",
    "y_valid = data_valid[\"class\"]\n",
    "y_test = data_test[\"class\"]\n",
    "\n",
    "# Encode the target variable (multi-class encoding)\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_y_train = encoder.transform(y_train)\n",
    "encoded_y_valid = encoder.transform(y_valid)\n",
    "encoded_y_test = encoder.transform(y_test)\n",
    "\n",
    "# Clean the data (replace NaNs and infinities)\n",
    "X_train, X_valid, X_test = clean_data(X_train, X_valid, X_test)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Function to build a model with variable layers and neurons\n",
    "def build_gru_model(\n",
    "    num_neurons=32, dropout_rate=0.2, learning_rate=0.001, input_shape=(100, 6)\n",
    "):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(num_neurons, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(GRU(num_neurons))\n",
    "    model.add(\n",
    "        Dense(4, activation=\"softmax\")\n",
    "    )  # 4 classes for multi-class classification\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Reshape data for the GRU model\n",
    "def reshape_data(X, window_length):\n",
    "    num_features = X.shape[1]\n",
    "    return X.reshape(X.shape[0], window_length, num_features // window_length)\n",
    "\n",
    "\n",
    "# Prepare the reshaped data\n",
    "window_length = 4  # Example window size\n",
    "X_train_reshaped = reshape_data(X_train, window_length)\n",
    "X_valid_reshaped = reshape_data(X_valid, window_length)\n",
    "X_test_reshaped = reshape_data(X_test, window_length)\n",
    "\n",
    "# Wrap the model in KerasClassifier for GridSearchCV\n",
    "model = KerasClassifier(\n",
    "    build_fn=build_gru_model,\n",
    "    input_shape=(window_length, X_train_reshaped.shape[2]),\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Define hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    \"batch_size\": [32, 64],\n",
    "    \"epochs\": [10, 20],\n",
    "    \"num_neurons\": [32, 64, 128],\n",
    "    \"dropout_rate\": [0.2, 0.3, 0.4],\n",
    "}\n",
    "\n",
    "# Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train_reshaped, encoded_y_train)\n",
    "\n",
    "# Output the best hyperparameters\n",
    "print(f\"Best Hyperparameters: {grid_result.best_params_}\")\n",
    "print(f\"Best Score: {grid_result.best_score_}\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = grid_result.best_estimator_\n",
    "test_accuracy = best_model.score(X_test_reshaped, encoded_y_test)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = best_model.predict(X_test_reshaped)\n",
    "cm = confusion_matrix(encoded_y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"g\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Load new data and predict (for grade 4)\n",
    "def combine_pickles_with_class(path, class_name):\n",
    "    acc_files = []\n",
    "    gyro_files = []\n",
    "    for file in os.listdir(path):\n",
    "        if file.startswith(class_name) and file.endswith(\"_acc.pkl\"):\n",
    "            acc_files.append(file)\n",
    "        elif file.startswith(class_name) and file.endswith(\"_gyro.pkl\"):\n",
    "            gyro_files.append(file)\n",
    "\n",
    "    combined_data_list = []\n",
    "    for i in range(len(acc_files)):\n",
    "        acc_df = pd.read_pickle(path + acc_files[i])\n",
    "        gyro_df = pd.read_pickle(path + gyro_files[i])\n",
    "\n",
    "        if len(acc_df.columns) == 4:\n",
    "            acc_df.drop(acc_df.columns[0], axis=1, inplace=True)\n",
    "            gyro_df.drop(gyro_df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "        acc_df.columns = [\"ax\", \"ay\", \"az\"]\n",
    "        gyro_df.columns = [\"gx\", \"gy\", \"gz\"]\n",
    "        combined_df = pd.concat([acc_df, gyro_df], axis=1)\n",
    "        combined_data_list.append(combined_df)\n",
    "\n",
    "    final_df = pd.concat(combined_data_list, ignore_index=True)\n",
    "    return final_df\n",
    "\n",
    "\n",
    "# Load the best saved model\n",
    "best_model = load_model(\"best_gru_model.h5\")\n",
    "\n",
    "# Load new data\n",
    "new_data = combine_pickles_with_class(\"newdata/\", \"data\")\n",
    "\n",
    "# Apply the same scaler used earlier\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "\n",
    "# Reshape the new data based on the same window_length used in training\n",
    "new_data_reshaped = reshape_data(new_data_scaled, window_length)\n",
    "\n",
    "# Predict on the new data using the best model\n",
    "new_pred = best_model.predict(new_data_reshaped)\n",
    "\n",
    "# Get the predicted classes\n",
    "new_pred_classes = np.argmax(new_pred, axis=1)\n",
    "print(new_pred_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
