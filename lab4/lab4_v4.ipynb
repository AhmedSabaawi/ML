{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to clean NaN and infinite values\n",
    "def clean_data(X_train, X_valid, X_test):\n",
    "    X_train.fillna(X_train.mean(), inplace=True)\n",
    "    X_valid.fillna(X_valid.mean(), inplace=True)\n",
    "    X_test.fillna(X_test.mean(), inplace=True)\n",
    "    return X_train, X_valid, X_test\n",
    "\n",
    "# Load data\n",
    "data_train = pd.read_pickle(\"clean_data/train/all_data_train.pkl\")\n",
    "data_valid = pd.read_pickle(\"clean_data/valid/all_data_valid.pkl\")\n",
    "data_test = pd.read_pickle(\"clean_data/test/all_data_test.pkl\")\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X_train = data_train.drop(\"class\", axis=1)\n",
    "X_valid = data_valid.drop(\"class\", axis=1)\n",
    "X_test = data_test.drop(\"class\", axis=1)\n",
    "\n",
    "y_train = data_train[\"class\"]\n",
    "y_valid = data_valid[\"class\"]\n",
    "y_test = data_test[\"class\"]\n",
    "\n",
    "# Encode the target variable (multi-class encoding)\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_y_train = encoder.transform(y_train)\n",
    "encoded_y_valid = encoder.transform(y_valid)\n",
    "encoded_y_test = encoder.transform(y_test)\n",
    "\n",
    "# Clean the data (replace NaNs and infinities)\n",
    "X_train, X_valid, X_test = clean_data(X_train, X_valid, X_test)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Function to adjust the window length based on the feature size\n",
    "def adjust_window_length(X, window_length):\n",
    "    num_features = X.shape[1]\n",
    "\n",
    "    # Ensure the window length does not exceed the number of features\n",
    "    if window_length > num_features:\n",
    "        print(f\"Window length {window_length} is too large for {num_features} features. Adjusting window length.\")\n",
    "        window_length = num_features\n",
    "\n",
    "    if num_features % window_length != 0:\n",
    "        new_window_length = num_features // (num_features // window_length)\n",
    "        print(f\"Adjusted window length from {window_length} to {new_window_length}\")\n",
    "        return new_window_length\n",
    "\n",
    "    return window_length\n",
    "\n",
    "# Function to build a GRU model with variable layers and neurons\n",
    "def build_gru_model(num_neurons=32, num_layers=1, dropout_rate=0.2, input_shape=(None,)):\n",
    "    model = Sequential()\n",
    "    for i in range(num_layers):\n",
    "        if i == 0:\n",
    "            model.add(GRU(num_neurons, return_sequences=(num_layers > 1), input_shape=input_shape))\n",
    "        else:\n",
    "            model.add(GRU(num_neurons, return_sequences=(i != num_layers - 1)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer for classification (4 classes)\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap the model for use with GridSearchCV\n",
    "model = KerasClassifier(build_fn=build_gru_model, input_shape=(X_train.shape[1],))\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'num_neurons': [32, 64, 128],\n",
    "    'num_layers': [1, 2],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3],\n",
    "    'epochs': [30],\n",
    "    'batch_size': [32, 64]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3, verbose=1)\n",
    "\n",
    "# Fit the grid search\n",
    "grid_result = grid.fit(X_train, encoded_y_train)\n",
    "\n",
    "# Display the best model and its parameters\n",
    "print(f\"Best Accuracy: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "\n",
    "# Evaluate the best model on validation data\n",
    "best_model = grid_result.best_estimator_.model\n",
    "_, accuracy = best_model.evaluate(X_valid, encoded_y_valid)\n",
    "print(f\"Validation Accuracy of Best Model: {accuracy}\")\n",
    "\n",
    "# Predict on the test data\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])  # Example reshape for 1 window length\n",
    "y_pred = np.argmax(best_model.predict(X_test_reshaped), axis=1)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(encoded_y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"g\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Function to combine pickles with class\n",
    "def combine_pickles_with_class(path, class_name):\n",
    "    acc_files = []\n",
    "    gyro_files = []\n",
    "\n",
    "    # List all files in the directory\n",
    "    for file in os.listdir(path):\n",
    "        if file.startswith(class_name) and file.endswith(\"_acc.pkl\"):\n",
    "            acc_files.append(file)\n",
    "        elif file.startswith(class_name) and file.endswith(\"_gyro.pkl\"):\n",
    "            gyro_files.append(file)\n",
    "\n",
    "    combined_data_list = []\n",
    "\n",
    "    # Process and combine acc and gyro files\n",
    "    for i in range(len(acc_files)):\n",
    "        acc_df = pd.read_pickle(os.path.join(path, acc_files[i]))\n",
    "        gyro_df = pd.read_pickle(os.path.join(path, gyro_files[i]))\n",
    "\n",
    "        # Drop the first column if there are 4 columns\n",
    "        if len(acc_df.columns) == 4:\n",
    "            acc_df.drop(acc_df.columns[0], axis=1, inplace=True)\n",
    "            gyro_df.drop(gyro_df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "        acc_df.columns = [\"ax\", \"ay\", \"az\"]\n",
    "        gyro_df.columns = [\"gx\", \"gy\", \"gz\"]\n",
    "\n",
    "        # Combine the acc and gyro dataframes\n",
    "        combined_df = pd.concat([acc_df, gyro_df], axis=1)\n",
    "\n",
    "        combined_data_list.append(combined_df)\n",
    "\n",
    "    # Concatenate all dataframes\n",
    "    final_df = pd.concat(combined_data_list, ignore_index=True)\n",
    "    return final_df\n",
    "\n",
    "# Load the best saved model and predict on new data\n",
    "new_data = combine_pickles_with_class(\"newdata/\", \"data\")\n",
    "\n",
    "# Apply the same scaler used earlier\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "\n",
    "# Reshape the new data based on the same window_length used in training\n",
    "new_data_reshaped = new_data_scaled.reshape(new_data_scaled.shape[0], 1, new_data_scaled.shape[1])\n",
    "\n",
    "# Predict on the new data using the best model\n",
    "new_pred = np.argmax(best_model.predict(new_data_reshaped), axis=1)\n",
    "\n",
    "# Plot raw data with predicted classes\n",
    "def plot_sensor_data(new_data, predicted_classes):\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "    # Plot accelerometer data\n",
    "    ax[0].plot(new_data.index, new_data['ax'], label='ax', color='b', linewidth=1.5)\n",
    "    ax[0].plot(new_data.index, new_data['ay'], label='ay', color='g', linewidth=1.5)\n",
    "    ax[0].plot(new_data.index, new_data['az'], label='az', color='r', linewidth=1.5)\n",
    "    ax[0].set_title(\"Accelerometer Data\")\n",
    "    ax[0].legend(loc=\"upper right\")\n",
    "\n",
    "    # Plot gyroscope data\n",
    "    ax[1].plot(new_data.index, new_data['gx'], label='gx', color='b', linewidth=1.5)\n",
    "    ax[1].plot(new_data.index, new_data['gy'], label='gy', color='g', linewidth=1.5)\n",
    "    ax[1].plot(new_data.index, new_data['gz'], label='gz', color='r', linewidth=1.5)\n",
    "    ax[1].set_title(\"Gyroscope Data\")\n",
    "    ax[1].legend(loc=\"upper right\")\n",
    "\n",
    "    # Adding vertical lines at points of class change\n",
    "    class_changes = [i for i in range(1, len(predicted_classes)) if predicted_classes[i - 1] != predicted_classes[i]]\n",
    "    for change_index in class_changes:\n",
    "        ax[0].axvline(x=new_data.index[change_index], color=\"k\", linestyle=\"--\", alpha=0.5)\n",
    "        ax[1].axvline(x=new_data.index[change_index], color=\"k\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the new data and predicted classes\n",
    "plot_sensor_data(new_data, new_pred)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
